import os
import pandas as pd
import numpy as np
import joblib
import json

from rdkit import Chem
from rdkit.Chem import Descriptors, Crippen, rdMolDescriptors, EState
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error

# === –®–ª—è—Ö –¥–æ CSV-—Ñ–∞–π–ª—É ===
data_path = 'D:/–∫—É—Ä—Å–æ–≤–∞/LogP_LogS.csv'

# === –ó–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –¥–∞—Ç–∞—Å–µ—Ç—É ===
df = pd.read_csv(data_path)

# –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
if 'SMILES' not in df.columns or 'LogS' not in df.columns:
    raise ValueError("CSV –ø–æ–≤–∏–Ω–µ–Ω –º—ñ—Å—Ç–∏—Ç–∏ –∫–æ–ª–æ–Ω–∫–∏ 'SMILES' —Ç–∞ 'LogS'")

# === –§—É–Ω–∫—Ü—ñ—è –¥–ª—è –≤–∏—Ç—è–≥–Ω–µ–Ω–Ω—è –¥–µ—Å–∫—Ä–∏–ø—Ç–æ—Ä—ñ–≤ ===
def extract_features(smiles):
    mol = Chem.MolFromSmiles(smiles)
    if mol is None:
        return None
    try:
        estate_indices = np.array(EState.EStateIndices(mol))
        return [
            Crippen.MolLogP(mol),  # SLogP
            Descriptors.MolLogP(mol),  # CLogP
            Crippen.MolLogP(mol),  # XLogP3 (—Ç–æ–π —Å–∞–º–∏–π –¥–ª—è —Å–ø—Ä–æ—â–µ–Ω–Ω—è)
            rdMolDescriptors.CalcTPSA(mol),
            rdMolDescriptors.CalcLabuteASA(mol),
            rdMolDescriptors.CalcTPSA(mol),
            rdMolDescriptors.CalcNumHBD(mol),
            rdMolDescriptors.CalcNumHBA(mol),
            Descriptors.NHOHCount(mol),
            Descriptors.BertzCT(mol),
            rdMolDescriptors.CalcNumRings(mol),
            Descriptors.NumAromaticRings(mol),
            Descriptors.FractionCSP3(mol),
            np.mean(estate_indices) if len(estate_indices) > 0 else 0.0,
            np.sum(estate_indices) if len(estate_indices) > 0 else 0.0
        ]
    except Exception as e:
        print(f"‚ùå Failed to extract features for {smiles}: {e}")
        return None


# === –û–±—á–∏—Å–ª–µ–Ω–Ω—è –æ–∑–Ω–∞–∫ ===
df['features'] = df['SMILES'].apply(extract_features)
df = df.dropna(subset=['features'])

# –ü–æ–±—É–¥–æ–≤–∞ –º–∞—Ç—Ä–∏—Ü—ñ –æ–∑–Ω–∞–∫ —ñ —Ü—ñ–ª—å–æ–≤–æ—ó –∑–º—ñ–Ω–Ω–æ—ó
feature_names = [
    "SLogP", "CLogP", "XLogP3", "TPSA", "ASA", "PSA",
    "HBD", "HBA", "NHOHCount", "BertzCT", "NumRings", "NumAromaticRings",
    "FractionCsp3", "Mean_EState", "Sum_EState"
]
X = pd.DataFrame(df['features'].tolist(), columns=feature_names)
y = df['LogS'].values

# === –†–æ–∑–¥—ñ–ª–µ–Ω–Ω—è –¥–∞–Ω–∏—Ö ===
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# === –ù–∞–≤—á–∞–Ω–Ω—è –º–æ–¥–µ–ª—ñ ===
model = RandomForestRegressor(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# === –ü—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è ===
y_pred = model.predict(X_test)

# === –û–±—á–∏—Å–ª–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ ===
metrics = {
    "r2_score": round(r2_score(y_test, y_pred), 4),
    "rmse": round(np.sqrt(mean_squared_error(y_test, y_pred)), 4),
    "mae": round(mean_absolute_error(y_test, y_pred), 4),
    "feature_importance": dict(zip(feature_names, model.feature_importances_.round(4).tolist()))
}

# === –°—Ç–≤–æ—Ä–µ–Ω–Ω—è –¥–∏—Ä–µ–∫—Ç–æ—Ä—ñ—ó ===
os.makedirs('models', exist_ok=True)

# === –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ ===
joblib.dump(model, 'models/latest_model.pkl')

# === –ó–±–µ—Ä–µ–∂–µ–Ω–Ω—è –º–µ—Ç—Ä–∏–∫ ===
with open('models/metrics.json', 'w') as f:
    json.dump(metrics, f, indent=2)

print("‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø—ñ—à–Ω–æ –Ω–∞–≤—á–µ–Ω–æ —Ç–∞ –∑–±–µ—Ä–µ–∂–µ–Ω–æ")
print("üìä –ú–µ—Ç—Ä–∏–∫–∏:")
print(json.dumps(metrics, indent=2))
